{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Name List\n",
    "model_name_list = [\"openai-community/gpt2-xl\", \"\", \"\",\n",
    "                   \"meta-llama/Llama-2-7b\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                   \"internlm/internlm2_5-7b-chat\", \"microsoft/Phi-3-small-128k-instruct\", \"01-ai/Yi-1.5-9B-Chat\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kimyw/bin/miniconda3/envs/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:04<00:00,  1.22s/it]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "\n",
    "model_name = model_name_list[0]\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map = 'balanced')\n",
    "\n",
    "tokenizer.save_pretrained(\"./models/\" + model_name)\n",
    "model.save_pretrained(\"./models/\" + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = model_name_list[5]\n",
    "print(\"model_name: \", model_name)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map = 'auto', trust_remote_code = True)\n",
    "\"\"\"\n",
    "tokenizer.save_pretrained(\"./models/\" + model_name)\n",
    "model.save_pretrained(\"./models/\" + model_name)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./models/\" + model_name, trust_remote_code = True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"./models/\" + model_name, device_map = 'auto', trust_remote_code = True)\n",
    "\"\"\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Report>\n",
      "The user is currently focused but surprised, which might be due to an unexpected feature or information. It's essential to understand what triggered this reaction and how it affected their experience.\n",
      "\n",
      "<UX EVALUATION>\n",
      "Q: What just caught your attention and caused the surprise?\n",
      "Q: Were you expecting to see that feature or information, or was it unexpected?\n",
      "Q: How did the surprise make you feel about the overall experience so far?\n"
     ]
    }
   ],
   "source": [
    "attention = \"focus\"\n",
    "emotion = \"surprise\"\n",
    "\n",
    "system_prompt = f\"You are a professional in UX usability evaluation designed to evaluate the user experience.\\n\\\n",
    "    The user is currently experiencing it in real time, and you are conducting a UX evaluation in real time. \\\n",
    "    You should ask questions based on the \\\"Current User Status\\\" to evaluate various UX factors such as UI/UX design, user interaction, and user satisfaction.\\n\\\n",
    "    \\n\\\n",
    "    ## Answering Rules\\n\\\n",
    "    1. Follow the \\\"Answering Form\\\"\\n\\\n",
    "    2. Only generate questions for <UX EVALUATION>.\\n\\\n",
    "    3. In <Report>, write a summary of the user's overall status and behavioral feedback to the user.\\n\\\n",
    "    4. Do not regenerate the \\\"Current User Status\\\".\\n\\\n",
    "    5. Generate the answer as a markdown language.\\n\\\n",
    "    \\n\\\n",
    "    ## Current User Status\\n\\\n",
    "    - Attention: {attention}\\n\\\n",
    "    - Emotion: {emotion}\\n\\\n",
    "    \\n\\\n",
    "    ## Examples\\n\\\n",
    "    Report Example 1 (When the user's \\\"Attention\\\" is in the drowsy state): You are dozing off now! Wake up!\\n\\\n",
    "    Question Example 1 (When the user's \\\"Attention\\\" is in the unfocus state): Did you feel like you had too many features or information?\\n\\\n",
    "    Question Example 2 (When the user's \\\"Emotion\\\" is in the angry state): What part of your use did you feel stressed about?\\n\\\n",
    "    Question Example 3 (When the user's \\\"Emotion\\\" is in the happy state): How satisfied were you after using it?\\n\\\n",
    "    \\n\\\n",
    "    ## Answering Form\\n\\\n",
    "    <Report>\\n\\\n",
    "    (Report)\\n\\\n",
    "    \\n\\\n",
    "    <UX EVALUATION>\\n\\\n",
    "    Q: (question)\\n\\\n",
    "    Q: (question)\\n\\\n",
    "    Q: (question)\\n\\\n",
    "    \"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"What are the most effective UX evaluation questions in the current situation?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = model_name_list[5]\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map = 'auto', trust_remote_code = True)\n",
    "\n",
    "\"\"\"\n",
    "tokenizer.save_pretrained(\"./models/\" + model_name)\n",
    "model.save_pretrained(\"./models/\" + model_name)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./models/\" + model_name)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"./models/\" + model_name)\n",
    "\"\"\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = \"focus\"\n",
    "emotion = \"surprise\"\n",
    "\n",
    "system_prompt = f\"You are a professional in UX usability evaluation designed to evaluate the user experience.\\n\\\n",
    "    The user is currently experiencing it in real time, and you are conducting a UX evaluation in real time. \\\n",
    "    You should ask questions based on the \\\"Current User Status\\\" to evaluate various UX factors such as UI/UX design, user interaction, and user satisfaction.\\n\\\n",
    "    ## Answering Rules\\n\\\n",
    "    1. Only generate questions for UX evaluation in line with \\\"Answering Form\\\" as an answer.\\n\\\n",
    "    2. Limit the generation to 250 tokens.\\n\\\n",
    "    3. Answer in Korean.\\n\\\n",
    "    ## Current User Status\\n\\\n",
    "    - Attention: {attention}\\n\\\n",
    "    - Emotion: {emotion}\\n\\\n",
    "    ## Examples\\n\\\n",
    "    Example 1 (When the user's \\\"Attention\\\" is in the unfocus state): Did you feel like you had too many features or information?\\n\\\n",
    "    Example 2 (When the user's \\\"Emotion\\\" is in the angry state): What part of your use did you feel stressed about?\\n\\\n",
    "    Example 3 (When the user's \\\"Emotion\\\" is in the happy state): How satisfied were you after using it?\\n\\\n",
    "    ## Answering Form\\n\\\n",
    "    1. Qusetion\\n\\\n",
    "    2. Qusetion\\n\\\n",
    "    ...\\n\\\n",
    "    \"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": \"What is the most appropriate question to evaluate UX in the current situation?\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "print(outputs)\n",
    "# print(outputs[0][\"generated_text\"][-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
