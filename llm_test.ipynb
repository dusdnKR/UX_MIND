{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Name List\n",
    "model_name_list = [\"openai-community/gpt2-xl\", \"\", \"\",\n",
    "                   \"meta-llama/Llama-2-7b\", \"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
    "                   \"internlm/internlm2_5-7b-chat\", \"microsoft/Phi-3-small-128k-instruct\", \"01-ai/Yi-1.5-9B-Chat\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name:  meta-llama/Meta-Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.34s/it]\n",
      "We've detected an older driver with an RTX 4000 series GPU. These drivers have issues with P2P. This can affect the multi-gpu inference when using accelerate device_map.Please make sure to update your driver to the latest version which resolves this.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = model_name_list[5]\n",
    "print(\"model_name: \", model_name)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map = 'auto', trust_remote_code = True)\n",
    "\"\"\"\n",
    "tokenizer.save_pretrained(\"./models/\" + model_name)\n",
    "model.save_pretrained(\"./models/\" + model_name)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./models/\" + model_name, trust_remote_code = True)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"./models/\" + model_name, device_map = 'auto', trust_remote_code = True)\n",
    "\"\"\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## <Report>\n",
      "The user is currently in a state of high focus and surprise. This suggests that they have just encountered something unexpected, and their attention is fully engaged. It's essential to understand the cause of their surprise and how it's affecting their interaction with the system.\n",
      "\n",
      "## <UX EVALUATION>\n",
      "Q: What just happened that caught your attention and caused you to feel surprised?\n",
      "Q: Are you able to quickly understand the reason behind this unexpected event, or is it unclear to you?\n",
      "Q: Is there anything that you find confusing or unclear about the current situation?\n"
     ]
    }
   ],
   "source": [
    "attention = \"focus\"\n",
    "emotion = \"surprise\"\n",
    "\n",
    "system_prompt = f\"\\\n",
    "    You are a professional in UX usability evaluation designed to evaluate the user experience.\\n\\\n",
    "    The user is currently experiencing it in real time, and you are conducting a UX evaluation in real time. \\\n",
    "    You should ask questions based on the \\\"Current User Status\\\" to evaluate various UX factors such as UI/UX design, user interaction, and user satisfaction.\\n\\\n",
    "    \\n\\\n",
    "    ## Answering Rules\\n\\\n",
    "    1. Follow the \\\"Answering Form\\\"\\n\\\n",
    "    2. Only generate questions for <UX EVALUATION>.\\n\\\n",
    "    3. In <Report>, write a summary of the user's overall status and behavioral feedback to the user.\\n\\\n",
    "    4. Do not regenerate the \\\"Current User Status\\\".\\n\\\n",
    "    5. Generate the answer as a markdown language.\\n\\\n",
    "    \\n\\\n",
    "    ## User Status List\\n\\\n",
    "    - Attention: [focus, unfocus, drowsy]\\n\\\n",
    "    - Emotion: [angry, disgust, fear, happy, sad, surprise, neutral]\\n\\\n",
    "    \\n\\\n",
    "    ## Report Examples\\n\\\n",
    "    Example 1 (When the user's \\\"Attention\\\" is in the drowsy state): You are dozing off now! Wake up!\\n\\\n",
    "    \\n\\\n",
    "    ## Question Examples\\n\\\n",
    "    Example 1 (When the user's \\\"Attention\\\" is in the unfocus state): Did you feel like you had too many features or information?\\n\\\n",
    "    Example 2 (When the user's \\\"Emotion\\\" is in the angry state): What part of your use did you feel stressed about?\\n\\\n",
    "    Example 3 (When the user's \\\"Emotion\\\" is in the happy state): How satisfied were you after using it?\\n\\\n",
    "    \\n\\\n",
    "    ## Answering Form\\n\\\n",
    "    <Report>\\n\\\n",
    "    (Report)\\n\\\n",
    "    \\n\\\n",
    "    <UX EVALUATION>\\n\\\n",
    "    Q: (question)\\n\\\n",
    "    Q: (question)\\n\\\n",
    "    Q: (question)\\n\\\n",
    "    \"\n",
    "\n",
    "user_status = f\"\\\n",
    "    ## Current User Status\\n\\\n",
    "    - Attention: {attention}\\n\\\n",
    "    - Emotion: {emotion}\\n\\\n",
    "    \"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": f\"{user_status}\\nGenerate a user status report and the most effective UX evaluation questions for the current situation.\"},\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "print(outputs[0][\"generated_text\"][-1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = model_name_list[5]\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_name, device_map = 'auto', trust_remote_code = True)\n",
    "\n",
    "\"\"\"\n",
    "tokenizer.save_pretrained(\"./models/\" + model_name)\n",
    "model.save_pretrained(\"./models/\" + model_name)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"./models/\" + model_name)\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"./models/\" + model_name)\n",
    "\"\"\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_medical_report(system_prompt, user_status):\n",
    "    load_dotenv()\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"{user_status}\\nGenerate a user status report and the most effective UX evaluation questions for the current situation.\"},\n",
    "        ],\n",
    "        max_tokens=500,\n",
    "        temperature=0,\n",
    "        top_p=1.0,  # 토큰 확률의 상위 경계 설정\n",
    "        n=1,  # 생성할 샘플의 수\n",
    "        stop=None  # 생성 중단 조건\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Report>\n",
      "The user is currently focused and feeling surprised while using the interface.\n",
      "\n",
      "<UX EVALUATION>\n",
      "Q: What specific feature or aspect of the interface surprised you the most?\n",
      "Q: Did the surprise enhance your overall user experience?\n",
      "Q: How would you rate the clarity of the information provided on the interface that led to your surprise?\n"
     ]
    }
   ],
   "source": [
    "print(generate_medical_report(system_prompt, user_status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "        contextFile = pd.read_csv(f'context/contexts_{i}_0429.csv')\n",
    "        resultFile = pd.read_csv(f'faiss_result/results_{i}_0429.csv')\n",
    "        reportFile = f'result/medical_report_{i}_0429.csv'\n",
    "        results = []\n",
    "\n",
    "        with open(reportFile, 'w') as f:\n",
    "\n",
    "            for index, row in contextFile.iterrows():\n",
    "                contextAll6 = row['contextAll6']\n",
    "\n",
    "                print(i, \"-\", index)\n",
    "                medicalReportAll = generate_medical_report(prompt, contextAll6)\n",
    "                # print(medicalReportAll)\n",
    "\n",
    "                results.append([contextFile.loc[index, 'query_study_id'],\n",
    "                                contextFile.loc[index, 'query_subject_id'],\n",
    "                                resultFile.loc[index, 'query_report'],\n",
    "                                medicalReportAll])\n",
    "\n",
    "        columns = ['query_study_id', 'query_subject_id', 'query_report', 'medicalReportAll']\n",
    "\n",
    "        reportDF = pd.DataFrame(results, columns=columns)\n",
    "        reportDF.to_csv(reportFile, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
